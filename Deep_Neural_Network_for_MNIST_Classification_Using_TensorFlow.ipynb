{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Neural Network for MNIST Classification Using TensorFlow",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a003c69bb1d484e8bc360ab6b2a343b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ac269c38c21c4dc68cc8bd4912b29704",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b31b17a4daf84a4faf6c9edf4cf2f805",
              "IPY_MODEL_6c6bb7ec9f9447849daceb34f1239c9f"
            ]
          }
        },
        "ac269c38c21c4dc68cc8bd4912b29704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b31b17a4daf84a4faf6c9edf4cf2f805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8389595cb2e8440c8365ef94837af8ab",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a6d4eb878ed4486a6704c16d015b2e6"
          }
        },
        "6c6bb7ec9f9447849daceb34f1239c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6cad0e0b28214db7a84f9f00e77302de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:01&lt;00:00,  3.14 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_132f5a3d111d4b7a8f97536b8d1f5c23"
          }
        },
        "8389595cb2e8440c8365ef94837af8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a6d4eb878ed4486a6704c16d015b2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cad0e0b28214db7a84f9f00e77302de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "132f5a3d111d4b7a8f97536b8d1f5c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mese9sFwPefR",
        "colab_type": "text"
      },
      "source": [
        "#Deep Neural Network for MNIST Classification Using TensorFlow\n",
        "\n",
        "The dataset is called MNIST and refers to handwritten digit recognition. We can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
        "\n",
        "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
        "\n",
        "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl4lX-rb5awr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KORbcAxS7kp4",
        "colab_type": "text"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZLZTVK17V8J",
        "colab_type": "code",
        "outputId": "db387a05-a960-441e-d3f4-fde410b42c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "5a003c69bb1d484e8bc360ab6b2a343b",
            "ac269c38c21c4dc68cc8bd4912b29704",
            "b31b17a4daf84a4faf6c9edf4cf2f805",
            "6c6bb7ec9f9447849daceb34f1239c9f",
            "8389595cb2e8440c8365ef94837af8ab",
            "2a6d4eb878ed4486a6704c16d015b2e6",
            "6cad0e0b28214db7a84f9f00e77302de",
            "132f5a3d111d4b7a8f97536b8d1f5c23"
          ]
        }
      },
      "source": [
        "# Downloading the data from Tensorflow_datasets\n",
        "mnist_dataset, mnist_info = tfds.load( name = \"mnist\", with_info=\"True\", as_supervised=\"True\")\n",
        "\n",
        "# Split the training and testing datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# Assigning number of validation samples needed\n",
        "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "# Calculating number of test samples present\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# Defining a function which takes an image & label and returned scaled image & label\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label\n",
        "\n",
        "# Custom transform the training dataset using .map function\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Similarly, transform the test dataset as well\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# Shuffle the training dataset\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Get the validation from the training data using .take method\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a003c69bb1d484e8bc360ab6b2a343b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILc4Mx2ZLDVB",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wniehU628q0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxF1uftpNNWS",
        "colab_type": "text"
      },
      "source": [
        "###Choose the optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsc10uuQNM1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYpWLX5uOex9",
        "colab_type": "text"
      },
      "source": [
        "###Training the built model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtFC0YMiOcvn",
        "colab_type": "code",
        "outputId": "386206f1-afd4-470e-970f-9b8a87a5c65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "NUM_EPOCHS = 5\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 10s - loss: 0.4085 - accuracy: 0.8848 - val_loss: 0.1963 - val_accuracy: 0.9455\n",
            "Epoch 2/5\n",
            "540/540 - 10s - loss: 0.1770 - accuracy: 0.9485 - val_loss: 0.1487 - val_accuracy: 0.9577\n",
            "Epoch 3/5\n",
            "540/540 - 11s - loss: 0.1353 - accuracy: 0.9605 - val_loss: 0.1237 - val_accuracy: 0.9628\n",
            "Epoch 4/5\n",
            "540/540 - 11s - loss: 0.1086 - accuracy: 0.9679 - val_loss: 0.1068 - val_accuracy: 0.9683\n",
            "Epoch 5/5\n",
            "540/540 - 11s - loss: 0.0923 - accuracy: 0.9727 - val_loss: 0.1021 - val_accuracy: 0.9688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5bf286a5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvSZGGKNPlrb",
        "colab_type": "text"
      },
      "source": [
        "###Testing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnzPfl-tPJEY",
        "colab_type": "code",
        "outputId": "16816dbb-c81b-4eaf-ad5e-9bed18e0fd03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNjbYHe_Pt09",
        "colab_type": "code",
        "outputId": "2c78a79c-97d4-40b6-9f38-09803d3cdd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Test loss: {0: .2f} & Test Accuracy: {1: .2f}%' .format(test_loss,test_accuracy*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss:  0.11 & Test Accuracy:  96.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySAEDNILDGBM",
        "colab_type": "text"
      },
      "source": [
        "##Variations of the above model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttukEtdYo4mO",
        "colab_type": "code",
        "outputId": "a8296440-8b40-4844-ed7e-a735c6aef8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Downloading the data from Tensorflow_datasets\n",
        "mnist_dataset, mnist_info = tfds.load( name = \"mnist\", with_info=\"True\", as_supervised=\"True\")\n",
        "\n",
        "# Split the training and testing datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# Assigning number of validation samples needed\n",
        "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "# Calculating number of test samples present\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# Defining a function which takes an image & label and returned scaled image & label\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label\n",
        "\n",
        "# Custom transform the training dataset using .map function\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Similarly, transform the test dataset as well\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# Shuffle the training dataset\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Get the validation from the training data using .take method\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n",
        "\n",
        "\n",
        "# Model\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n",
        "\n",
        "# Optimizer and loss function\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
        "\n",
        "# Testing the data\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print('Test loss: {0: .2f} & Test Accuracy: {1: .2f}%' .format(test_loss,test_accuracy*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1080/1080 - 12s - loss: 0.3377 - accuracy: 0.8995 - val_loss: 0.1755 - val_accuracy: 0.9468\n",
            "Epoch 2/5\n",
            "1080/1080 - 12s - loss: 0.1507 - accuracy: 0.9540 - val_loss: 0.1282 - val_accuracy: 0.9602\n",
            "Epoch 3/5\n",
            "1080/1080 - 12s - loss: 0.1142 - accuracy: 0.9655 - val_loss: 0.1148 - val_accuracy: 0.9653\n",
            "Epoch 4/5\n",
            "1080/1080 - 12s - loss: 0.0945 - accuracy: 0.9707 - val_loss: 0.0894 - val_accuracy: 0.9738\n",
            "Epoch 5/5\n",
            "1080/1080 - 12s - loss: 0.0791 - accuracy: 0.9753 - val_loss: 0.0845 - val_accuracy: 0.9747\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9696\n",
            "Test loss:  0.10 & Test Accuracy:  96.96%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dDtH9tSq9G8",
        "colab_type": "code",
        "outputId": "e70d209b-dddc-4266-b689-ae6b7b15fcff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "# Downloading the data from Tensorflow_datasets\n",
        "mnist_dataset, mnist_info = tfds.load( name = \"mnist\", with_info=\"True\", as_supervised=\"True\")\n",
        "\n",
        "# Split the training and testing datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# Assigning number of validation samples needed\n",
        "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "# Calculating number of test samples present\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# Defining a function which takes an image & label and returned scaled image & label\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label\n",
        "\n",
        "# Custom transform the training dataset using .map function\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Similarly, transform the test dataset as well\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# Shuffle the training dataset\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Get the validation from the training data using .take method\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 1000\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n",
        "\n",
        "\n",
        "# Model\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n",
        "\n",
        "# Optimizer and loss function\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
        "\n",
        "# Testing the data\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print('Test loss: {0: .2f} & Test Accuracy: {1: .2f}%' .format(test_loss,test_accuracy*100))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 - 10s - loss: 0.9019 - accuracy: 0.7490 - val_loss: 0.3233 - val_accuracy: 0.9033\n",
            "Epoch 2/10\n",
            "54/54 - 10s - loss: 0.2577 - accuracy: 0.9244 - val_loss: 0.2105 - val_accuracy: 0.9373\n",
            "Epoch 3/10\n",
            "54/54 - 10s - loss: 0.1832 - accuracy: 0.9461 - val_loss: 0.1572 - val_accuracy: 0.9528\n",
            "Epoch 4/10\n",
            "54/54 - 10s - loss: 0.1421 - accuracy: 0.9574 - val_loss: 0.1278 - val_accuracy: 0.9620\n",
            "Epoch 5/10\n",
            "54/54 - 10s - loss: 0.1169 - accuracy: 0.9655 - val_loss: 0.1111 - val_accuracy: 0.9643\n",
            "Epoch 6/10\n",
            "54/54 - 10s - loss: 0.1018 - accuracy: 0.9691 - val_loss: 0.0957 - val_accuracy: 0.9705\n",
            "Epoch 7/10\n",
            "54/54 - 10s - loss: 0.0827 - accuracy: 0.9752 - val_loss: 0.0899 - val_accuracy: 0.9730\n",
            "Epoch 8/10\n",
            "54/54 - 10s - loss: 0.0756 - accuracy: 0.9775 - val_loss: 0.0814 - val_accuracy: 0.9748\n",
            "Epoch 9/10\n",
            "54/54 - 10s - loss: 0.0689 - accuracy: 0.9787 - val_loss: 0.0785 - val_accuracy: 0.9753\n",
            "Epoch 10/10\n",
            "54/54 - 10s - loss: 0.0616 - accuracy: 0.9813 - val_loss: 0.0620 - val_accuracy: 0.9812\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9705\n",
            "Test loss:  0.10 & Test Accuracy:  97.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A9XnN8ls5r2",
        "colab_type": "code",
        "outputId": "06d2630e-9a6b-49d5-ca30-57cef9497cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "# Downloading the data from Tensorflow_datasets\n",
        "mnist_dataset, mnist_info = tfds.load( name = \"mnist\", with_info=\"True\", as_supervised=\"True\")\n",
        "\n",
        "# Split the training and testing datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# Assigning number of validation samples needed\n",
        "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "# Calculating number of test samples present\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# Defining a function which takes an image & label and returned scaled image & label\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label\n",
        "\n",
        "# Custom transform the training dataset using .map function\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Similarly, transform the test dataset as well\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# Shuffle the training dataset\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Get the validation from the training data using .take method\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 200\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n",
        "\n",
        "\n",
        "# Model\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n",
        "\n",
        "# Optimizer and loss function\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
        "\n",
        "# Testing the data\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print('Test loss: {0: .2f} & Test Accuracy: {1: .2f}%' .format(test_loss,test_accuracy*100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "270/270 - 10s - loss: 0.5550 - accuracy: 0.8297 - val_loss: 0.2444 - val_accuracy: 0.9275\n",
            "Epoch 2/10\n",
            "270/270 - 10s - loss: 0.1973 - accuracy: 0.9407 - val_loss: 0.1621 - val_accuracy: 0.9503\n",
            "Epoch 3/10\n",
            "270/270 - 10s - loss: 0.1472 - accuracy: 0.9556 - val_loss: 0.1249 - val_accuracy: 0.9643\n",
            "Epoch 4/10\n",
            "270/270 - 10s - loss: 0.1231 - accuracy: 0.9629 - val_loss: 0.1116 - val_accuracy: 0.9663\n",
            "Epoch 5/10\n",
            "270/270 - 10s - loss: 0.1014 - accuracy: 0.9693 - val_loss: 0.1030 - val_accuracy: 0.9698\n",
            "Epoch 6/10\n",
            "270/270 - 10s - loss: 0.0881 - accuracy: 0.9732 - val_loss: 0.0834 - val_accuracy: 0.9758\n",
            "Epoch 7/10\n",
            "270/270 - 10s - loss: 0.0801 - accuracy: 0.9752 - val_loss: 0.0781 - val_accuracy: 0.9762\n",
            "Epoch 8/10\n",
            "270/270 - 10s - loss: 0.0692 - accuracy: 0.9789 - val_loss: 0.0742 - val_accuracy: 0.9783\n",
            "Epoch 9/10\n",
            "270/270 - 10s - loss: 0.0668 - accuracy: 0.9789 - val_loss: 0.0700 - val_accuracy: 0.9792\n",
            "Epoch 10/10\n",
            "270/270 - 10s - loss: 0.0546 - accuracy: 0.9828 - val_loss: 0.0705 - val_accuracy: 0.9787\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9676\n",
            "Test loss:  0.11 & Test Accuracy:  96.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hnxc5MHti18",
        "colab_type": "code",
        "outputId": "de247fb4-0275-4c7b-dde8-d803eacc600a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "# Downloading the data from Tensorflow_datasets\n",
        "mnist_dataset, mnist_info = tfds.load( name = \"mnist\", with_info=\"True\", as_supervised=\"True\")\n",
        "\n",
        "# Split the training and testing datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# Assigning number of validation samples needed\n",
        "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "# Calculating number of test samples present\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# Defining a function which takes an image & label and returned scaled image & label\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label\n",
        "\n",
        "# Custom transform the training dataset using .map function\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Similarly, transform the test dataset as well\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# Shuffle the training dataset\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Get the validation from the training data using .take method\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n",
        "\n",
        "\n",
        "# Model\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n",
        "\n",
        "# Optimizer and loss function\n",
        "custom_adam = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer= custom_adam, loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
        "\n",
        "# Testing the data\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print('Test loss: {0: .2f} & Test Accuracy: {1: .2f}%' .format(test_loss,test_accuracy*100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "540/540 - 11s - loss: 1.2404 - accuracy: 0.6461 - val_loss: 0.4691 - val_accuracy: 0.8782\n",
            "Epoch 2/10\n",
            "540/540 - 11s - loss: 0.3851 - accuracy: 0.8942 - val_loss: 0.3155 - val_accuracy: 0.9122\n",
            "Epoch 3/10\n",
            "540/540 - 11s - loss: 0.2971 - accuracy: 0.9149 - val_loss: 0.2640 - val_accuracy: 0.9265\n",
            "Epoch 4/10\n",
            "540/540 - 11s - loss: 0.2596 - accuracy: 0.9253 - val_loss: 0.2341 - val_accuracy: 0.9353\n",
            "Epoch 5/10\n",
            "540/540 - 11s - loss: 0.2330 - accuracy: 0.9328 - val_loss: 0.2160 - val_accuracy: 0.9422\n",
            "Epoch 6/10\n",
            "540/540 - 11s - loss: 0.2145 - accuracy: 0.9383 - val_loss: 0.2034 - val_accuracy: 0.9432\n",
            "Epoch 7/10\n",
            "540/540 - 11s - loss: 0.1989 - accuracy: 0.9420 - val_loss: 0.1867 - val_accuracy: 0.9467\n",
            "Epoch 8/10\n",
            "540/540 - 11s - loss: 0.1867 - accuracy: 0.9457 - val_loss: 0.1743 - val_accuracy: 0.9512\n",
            "Epoch 9/10\n",
            "540/540 - 11s - loss: 0.1730 - accuracy: 0.9490 - val_loss: 0.1640 - val_accuracy: 0.9552\n",
            "Epoch 10/10\n",
            "540/540 - 11s - loss: 0.1632 - accuracy: 0.9524 - val_loss: 0.1562 - val_accuracy: 0.9547\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9521\n",
            "Test loss:  0.16 & Test Accuracy:  95.21%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezVpU1R6ul-9",
        "colab_type": "code",
        "outputId": "fe1e66d6-a914-4e55-fa6c-92395d4f15ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "# Downloading the data from Tensorflow_datasets\n",
        "mnist_dataset, mnist_info = tfds.load( name = \"mnist\", with_info=\"True\", as_supervised=\"True\")\n",
        "\n",
        "# Split the training and testing datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# Assigning number of validation samples needed\n",
        "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "# Calculating number of test samples present\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# Defining a function which takes an image & label and returned scaled image & label\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label\n",
        "\n",
        "# Custom transform the training dataset using .map function\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Similarly, transform the test dataset as well\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# Shuffle the training dataset\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Get the validation from the training data using .take method\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n",
        "\n",
        "\n",
        "# Model\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n",
        "\n",
        "# Optimizer and loss function\n",
        "custom_adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer= custom_adam, loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
        "\n",
        "# Testing the data\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print('Test loss: {0: .2f} & Test Accuracy: {1: .2f}%' .format(test_loss,test_accuracy*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "540/540 - 11s - loss: 0.4377 - accuracy: 0.8641 - val_loss: 0.1879 - val_accuracy: 0.9463\n",
            "Epoch 2/20\n",
            "540/540 - 11s - loss: 0.1643 - accuracy: 0.9510 - val_loss: 0.1480 - val_accuracy: 0.9583\n",
            "Epoch 3/20\n",
            "540/540 - 11s - loss: 0.1273 - accuracy: 0.9614 - val_loss: 0.1132 - val_accuracy: 0.9668\n",
            "Epoch 4/20\n",
            "540/540 - 11s - loss: 0.1059 - accuracy: 0.9677 - val_loss: 0.1049 - val_accuracy: 0.9692\n",
            "Epoch 5/20\n",
            "540/540 - 11s - loss: 0.0906 - accuracy: 0.9722 - val_loss: 0.0938 - val_accuracy: 0.9730\n",
            "Epoch 6/20\n",
            "540/540 - 12s - loss: 0.0792 - accuracy: 0.9748 - val_loss: 0.0924 - val_accuracy: 0.9718\n",
            "Epoch 7/20\n",
            "540/540 - 11s - loss: 0.0712 - accuracy: 0.9776 - val_loss: 0.0801 - val_accuracy: 0.9782\n",
            "Epoch 8/20\n",
            "540/540 - 11s - loss: 0.0612 - accuracy: 0.9807 - val_loss: 0.0730 - val_accuracy: 0.9792\n",
            "Epoch 9/20\n",
            "540/540 - 11s - loss: 0.0576 - accuracy: 0.9824 - val_loss: 0.0720 - val_accuracy: 0.9805\n",
            "Epoch 10/20\n",
            "540/540 - 11s - loss: 0.0508 - accuracy: 0.9836 - val_loss: 0.0724 - val_accuracy: 0.9787\n",
            "Epoch 11/20\n",
            "540/540 - 11s - loss: 0.0446 - accuracy: 0.9856 - val_loss: 0.0688 - val_accuracy: 0.9812\n",
            "Epoch 12/20\n",
            "540/540 - 12s - loss: 0.0388 - accuracy: 0.9875 - val_loss: 0.0559 - val_accuracy: 0.9845\n",
            "Epoch 13/20\n",
            "540/540 - 12s - loss: 0.0357 - accuracy: 0.9884 - val_loss: 0.0613 - val_accuracy: 0.9823\n",
            "Epoch 14/20\n",
            "540/540 - 11s - loss: 0.0366 - accuracy: 0.9889 - val_loss: 0.0531 - val_accuracy: 0.9838\n",
            "Epoch 15/20\n",
            "540/540 - 11s - loss: 0.0340 - accuracy: 0.9894 - val_loss: 0.0528 - val_accuracy: 0.9852\n",
            "Epoch 16/20\n",
            "540/540 - 11s - loss: 0.0284 - accuracy: 0.9912 - val_loss: 0.0604 - val_accuracy: 0.9833\n",
            "Epoch 17/20\n",
            "540/540 - 11s - loss: 0.0299 - accuracy: 0.9906 - val_loss: 0.0380 - val_accuracy: 0.9887\n",
            "Epoch 18/20\n",
            "540/540 - 12s - loss: 0.0295 - accuracy: 0.9902 - val_loss: 0.0575 - val_accuracy: 0.9823\n",
            "Epoch 19/20\n",
            "540/540 - 12s - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.0371 - val_accuracy: 0.9888\n",
            "Epoch 20/20\n",
            "540/540 - 12s - loss: 0.0207 - accuracy: 0.9935 - val_loss: 0.0334 - val_accuracy: 0.9900\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9735\n",
            "Test loss:  0.13 & Test Accuracy:  97.35%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnKTrgaRwNm2",
        "colab_type": "code",
        "outputId": "24348f68-3a3a-4280-e5d3-23a02e5f8974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Downloading the data from Tensorflow_datasets\n",
        "mnist_dataset, mnist_info = tfds.load( name = \"mnist\", with_info=\"True\", as_supervised=\"True\")\n",
        "\n",
        "# Split the training and testing datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "# Assigning number of validation samples needed\n",
        "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "# Calculating number of test samples present\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "# Defining a function which takes an image & label and returned scaled image & label\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  return image, label\n",
        "\n",
        "# Custom transform the training dataset using .map function\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "# Similarly, transform the test dataset as well\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "# Shuffle the training dataset\n",
        "BUFFER_SIZE = 10000\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Get the validation from the training data using .take method\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n",
        "\n",
        "\n",
        "# Model\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n",
        "\n",
        "# Optimizer and loss function\n",
        "custom_adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer= custom_adam, loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
        "\n",
        "# Testing the data\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print('Test loss: {0: .2f} & Test Accuracy: {1: .2f}%' .format(test_loss,test_accuracy*100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "540/540 - 11s - loss: 0.4113 - accuracy: 0.8790 - val_loss: 0.1986 - val_accuracy: 0.9403\n",
            "Epoch 2/30\n",
            "540/540 - 11s - loss: 0.1695 - accuracy: 0.9491 - val_loss: 0.1351 - val_accuracy: 0.9612\n",
            "Epoch 3/30\n",
            "540/540 - 12s - loss: 0.1261 - accuracy: 0.9620 - val_loss: 0.1283 - val_accuracy: 0.9622\n",
            "Epoch 4/30\n",
            "540/540 - 11s - loss: 0.1051 - accuracy: 0.9684 - val_loss: 0.0970 - val_accuracy: 0.9720\n",
            "Epoch 5/30\n",
            "540/540 - 11s - loss: 0.0841 - accuracy: 0.9740 - val_loss: 0.0881 - val_accuracy: 0.9763\n",
            "Epoch 6/30\n",
            "540/540 - 12s - loss: 0.0765 - accuracy: 0.9763 - val_loss: 0.0798 - val_accuracy: 0.9783\n",
            "Epoch 7/30\n",
            "540/540 - 11s - loss: 0.0666 - accuracy: 0.9793 - val_loss: 0.0675 - val_accuracy: 0.9807\n",
            "Epoch 8/30\n",
            "540/540 - 11s - loss: 0.0585 - accuracy: 0.9814 - val_loss: 0.0687 - val_accuracy: 0.9788\n",
            "Epoch 9/30\n",
            "540/540 - 11s - loss: 0.0506 - accuracy: 0.9841 - val_loss: 0.0618 - val_accuracy: 0.9812\n",
            "Epoch 10/30\n",
            "540/540 - 11s - loss: 0.0483 - accuracy: 0.9841 - val_loss: 0.0540 - val_accuracy: 0.9863\n",
            "Epoch 11/30\n",
            "540/540 - 11s - loss: 0.0425 - accuracy: 0.9865 - val_loss: 0.0572 - val_accuracy: 0.9813\n",
            "Epoch 12/30\n",
            "540/540 - 11s - loss: 0.0411 - accuracy: 0.9861 - val_loss: 0.0567 - val_accuracy: 0.9837\n",
            "Epoch 13/30\n",
            "540/540 - 11s - loss: 0.0359 - accuracy: 0.9880 - val_loss: 0.0480 - val_accuracy: 0.9863\n",
            "Epoch 14/30\n",
            "540/540 - 11s - loss: 0.0327 - accuracy: 0.9893 - val_loss: 0.0444 - val_accuracy: 0.9870\n",
            "Epoch 15/30\n",
            "540/540 - 11s - loss: 0.0297 - accuracy: 0.9904 - val_loss: 0.0491 - val_accuracy: 0.9840\n",
            "Epoch 16/30\n",
            "540/540 - 11s - loss: 0.0307 - accuracy: 0.9900 - val_loss: 0.0462 - val_accuracy: 0.9855\n",
            "Epoch 17/30\n",
            "540/540 - 11s - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.0544 - val_accuracy: 0.9850\n",
            "Epoch 18/30\n",
            "540/540 - 11s - loss: 0.0264 - accuracy: 0.9914 - val_loss: 0.0373 - val_accuracy: 0.9892\n",
            "Epoch 19/30\n",
            "540/540 - 11s - loss: 0.0261 - accuracy: 0.9912 - val_loss: 0.0295 - val_accuracy: 0.9907\n",
            "Epoch 20/30\n",
            "540/540 - 11s - loss: 0.0197 - accuracy: 0.9938 - val_loss: 0.0303 - val_accuracy: 0.9907\n",
            "Epoch 21/30\n",
            "540/540 - 11s - loss: 0.0208 - accuracy: 0.9929 - val_loss: 0.0308 - val_accuracy: 0.9905\n",
            "Epoch 22/30\n",
            "540/540 - 11s - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.0347 - val_accuracy: 0.9887\n",
            "Epoch 23/30\n",
            "540/540 - 11s - loss: 0.0172 - accuracy: 0.9943 - val_loss: 0.0336 - val_accuracy: 0.9895\n",
            "Epoch 24/30\n",
            "540/540 - 11s - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.0241 - val_accuracy: 0.9928\n",
            "Epoch 25/30\n",
            "540/540 - 11s - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.0275 - val_accuracy: 0.9920\n",
            "Epoch 26/30\n",
            "540/540 - 11s - loss: 0.0168 - accuracy: 0.9941 - val_loss: 0.0292 - val_accuracy: 0.9908\n",
            "Epoch 27/30\n",
            "540/540 - 11s - loss: 0.0159 - accuracy: 0.9945 - val_loss: 0.0243 - val_accuracy: 0.9923\n",
            "Epoch 28/30\n",
            "540/540 - 12s - loss: 0.0147 - accuracy: 0.9950 - val_loss: 0.0154 - val_accuracy: 0.9948\n",
            "Epoch 29/30\n",
            "540/540 - 11s - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0179 - val_accuracy: 0.9938\n",
            "Epoch 30/30\n",
            "540/540 - 11s - loss: 0.0145 - accuracy: 0.9953 - val_loss: 0.0233 - val_accuracy: 0.9930\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9744\n",
            "Test loss:  0.14 & Test Accuracy:  97.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzzjpZmy_uk4",
        "colab_type": "text"
      },
      "source": [
        "##Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eie3JIRs_z0K",
        "colab_type": "text"
      },
      "source": [
        "No matter how much we changed the values of hyperparameters of the model, we got the test accuracy as close to 97%."
      ]
    }
  ]
}